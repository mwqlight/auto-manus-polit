#!/bin/bash
# solve.sh - å®ŒæˆABMILæ¨¡å‹å®ç°çš„è‡ªåŠ¨åŒ–è„šæœ¬

set -e  # é‡åˆ°é”™è¯¯æ—¶é€€å‡º
set -u  # é‡åˆ°æœªå®šä¹‰å˜é‡æ—¶é€€å‡º

echo "ğŸš€ å¼€å§‹å®ŒæˆABMILæ¨¡å‹å®ç°..."

# 1. å¤‡ä»½åŸå§‹æ–‡ä»¶
BACKUP_DIR="/app/backups"
mkdir -p "$BACKUP_DIR"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
cp /app/abmil_assignment.py "${BACKUP_DIR}/abmil_assignment_backup_${TIMESTAMP}.py"
echo "âœ… åŸå§‹æ–‡ä»¶å·²å¤‡ä»½åˆ° ${BACKUP_DIR}/abmil_assignment_backup_${TIMESTAMP}.py"

# 2. ç”Ÿæˆå®Œæ•´çš„ABMILå®ç°
cat > /app/abmil_assignment.py << 'EOF'
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple, Union, List
import math
import warnings

class Attention_TanhSigmoidGating(nn.Module):
    """
    åŸºäºTanh-Sigmoidé—¨æ§æœºåˆ¶çš„æ³¨æ„åŠ›æ¨¡å—ï¼Œç”¨äºABMILæ¨¡å‹
    
    å‚è€ƒ: Ilse, M., Tomczak, J., & Welling, M. (2018). 
    Attention-based deep multiple instance learning.
    """
    
    def __init__(self, D: int, L: int, dropout: float = 0.25):
        """
        åˆå§‹åŒ–æ³¨æ„åŠ›æ¨¡å—
        
        å‚æ•°:
            D: è¾“å…¥ç‰¹å¾ç»´åº¦
            L: æ³¨æ„åŠ›ç½‘ç»œéšè—å±‚ç»´åº¦
            dropout: Dropoutæ¯”ç‡
        """
        super(Attention_TanhSigmoidGating, self).__init__()
        
        # éªŒè¯è¾“å…¥å‚æ•°
        if D <= 0 or L <= 0:
            raise ValueError("ç»´åº¦å‚æ•°Då’ŒLå¿…é¡»ä¸ºæ­£æ•´æ•°")
        if not (0 <= dropout < 1):
            raise ValueError("Dropoutæ¯”ç‡å¿…é¡»åœ¨[0, 1)èŒƒå›´å†…")
        
        # åˆ›å»ºç½‘ç»œå±‚ - ä½¿ç”¨æŒ‡å®šçš„ç¡®åˆ‡å±æ€§å
        self.tanhV = nn.Sequential(
            nn.Linear(D, L),
            nn.Tanh(),
            nn.Dropout(dropout)
        )
        
        self.sigmU = nn.Sequential(
            nn.Linear(D, L),
            nn.Sigmoid(),
            nn.Dropout(dropout)
        )
        
        self.w = nn.Linear(L, 1)
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for layer in self.tanhV:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_normal_(layer.weight)
                nn.init.constant_(layer.bias, 0)
        
        for layer in self.sigmU:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_normal_(layer.weight)
                nn.init.constant_(layer.bias, 0)
        
        nn.init.xavier_normal_(self.w.weight)
        nn.init.constant_(self.w.bias, 0)
    
    def _get_eps(self, dtype: torch.dtype) -> float:
        """
        æ ¹æ®æ•°æ®ç±»å‹è¿”å›é€‚å½“çš„epsilonå€¼ï¼Œç”¨äºæ•°å€¼ç¨³å®šæ€§
        
        å‚æ•°:
            dtype: å¼ é‡æ•°æ®ç±»å‹
            
        è¿”å›:
            é€‚å½“çš„epsilonå€¼
        """
        if dtype == torch.float16:
            return 1e-2
        elif dtype == torch.float32:
            return 1e-6
        elif dtype == torch.float64:
            return 1e-10
        else:
            return 1e-8
    
    def forward(self, H: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­è®¡ç®—æ³¨æ„åŠ›æƒé‡å’ŒåŠ æƒç‰¹å¾
        
        å‚æ•°:
            H: å½¢çŠ¶ä¸º(B, M, D)çš„å¼ é‡ï¼Œå…¶ä¸­
               B = batchå¤§å°
               M = è¢‹ä¸­å®ä¾‹(patch)æ•°é‡
               D = ç‰¹å¾ç»´åº¦
        
        è¿”å›:
            A: å½¢çŠ¶ä¸º(B, M)çš„æ³¨æ„åŠ›æƒé‡
            M: å½¢çŠ¶ä¸º(B, D)çš„åŠ æƒèšåˆç‰¹å¾
        """
        # è¾“å…¥éªŒè¯
        if not torch.is_tensor(H):
            raise TypeError("è¾“å…¥å¿…é¡»æ˜¯torch.Tensorç±»å‹")
        
        if H.dim() != 3:
            raise ValueError(f"è¾“å…¥å¼ é‡ç»´åº¦åº”ä¸º3 (batch, instances, features), ä½†å¾—åˆ° {H.dim()}")
        
        if torch.isnan(H).any() or torch.isinf(H).any():
            warnings.warn("è¾“å…¥å¼ é‡åŒ…å«NaNæˆ–Infå€¼", RuntimeWarning)
        
        # è·å–è®¾å¤‡å’Œæ•°æ®ç±»å‹
        device = H.device
        dtype = H.dtype
        
        # è·å–æ‰¹æ¬¡å¤§å°å’Œå®ä¾‹æ•°é‡
        B, M, D = H.shape
        
        # é‡å¡‘ä¸º2Då¼ é‡ä»¥è¿›è¡Œé«˜æ•ˆå¤„ç†
        H_2d = H.view(-1, D)  # (B*M, D)
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        tanh_part = self.tanhV(H_2d)  # (B*M, L)
        sigm_part = self.sigmU(H_2d)  # (B*M, L)
        gated = tanh_part * sigm_part  # (B*M, L)
        attn_scores = self.w(gated)    # (B*M, 1)
        
        # é‡å¡‘å›åŸå§‹æ‰¹æ¬¡ç»“æ„
        attn_scores = attn_scores.view(B, M)  # (B, M)
        
        # æ•°å€¼ç¨³å®šæ€§: æ ¹æ®æ•°æ®ç±»å‹è®¾ç½®é€‚å½“çš„epsilon
        eps = self._get_eps(dtype)
        
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡ (ä½¿ç”¨softmax)
        # ä¸ºé¿å…æ•°å€¼ä¸ç¨³å®šï¼Œå‡å»æœ€å¤§å€¼
        max_scores = attn_scores.max(dim=1, keepdim=True)[0]
        exp_scores = torch.exp(attn_scores - max_scores)
        A = exp_scores / (torch.sum(exp_scores, dim=1, keepdim=True) + eps)
        
        # è®¡ç®—åŠ æƒèšåˆç‰¹å¾
        M = torch.bmm(A.unsqueeze(1), H).squeeze(1)  # (B, D)
        
        # ç¡®ä¿è¾“å‡ºæ²¡æœ‰NaNæˆ–Inf
        if torch.isnan(A).any() or torch.isinf(A).any():
            warnings.warn("æ³¨æ„åŠ›æƒé‡åŒ…å«NaNæˆ–Infå€¼", RuntimeWarning)
        
        if torch.isnan(M).any() or torch.isinf(M).any():
            warnings.warn("èšåˆç‰¹å¾åŒ…å«NaNæˆ–Infå€¼", RuntimeWarning)
        
        return A, M


class ABMIL(nn.Module):
    """
    Attention-Based Multiple Instance Learning (ABMIL) æ¨¡å‹
    
    å‚è€ƒ: Ilse, M., Tomczak, J., & Welling, M. (2018). 
    Attention-based deep multiple instance learning.
    """
    
    def __init__(self, input_dim: int, hidden_dim: int, n_classes: int, dropout: float = 0.25):
        """
        åˆå§‹åŒ–ABMILæ¨¡å‹
        
        å‚æ•°:
            input_dim: è¾“å…¥ç‰¹å¾ç»´åº¦
            hidden_dim: éšè—å±‚ç»´åº¦
            n_classes: åˆ†ç±»ç±»åˆ«æ•°
            dropout: Dropoutæ¯”ç‡
        """
        super(ABMIL, self).__init__()
        
        # éªŒè¯è¾“å…¥å‚æ•°
        if input_dim <= 0 or hidden_dim <= 0 or n_classes <= 0:
            raise ValueError("æ‰€æœ‰ç»´åº¦å‚æ•°å¿…é¡»ä¸ºæ­£æ•´æ•°")
        if not (0 <= dropout < 1):
            raise ValueError("Dropoutæ¯”ç‡å¿…é¡»åœ¨[0, 1)èŒƒå›´å†…")
        
        # åˆ›å»ºæ¨¡å‹ç»„ä»¶ - ä½¿ç”¨æŒ‡å®šçš„ç¡®åˆ‡å±æ€§å
        self.inst_level_fc = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        self.global_attn = Attention_TanhSigmoidGating(D=hidden_dim, L=hidden_dim, dropout=dropout)
        
        self.bag_level_classifier = nn.Linear(hidden_dim, n_classes)
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
        for layer in self.inst_level_fc:
            if isinstance(layer, nn.Linear):
                nn.init.xavier_normal_(layer.weight)
                nn.init.constant_(layer.bias, 0)
        
        nn.init.xavier_normal_(self.bag_level_classifier.weight)
        nn.init.constant_(self.bag_level_classifier.bias, 0)
    
    def forward(self, x: torch.Tensor, lens: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‰å‘ä¼ æ’­è®¡ç®—åˆ†ç±»ç»“æœ
        
        å‚æ•°:
            x: å½¢çŠ¶ä¸º(B, M_max, D)çš„å¼ é‡ï¼ŒåŒ…å«å¡«å……åçš„bagç‰¹å¾
               B = batchå¤§å°
               M_max = æœ€å¤§bagå¤§å°
               D = ç‰¹å¾ç»´åº¦
            lens: å½¢çŠ¶ä¸º(B,)çš„å¼ é‡ï¼ŒåŒ…å«æ¯ä¸ªbagçš„å®é™…é•¿åº¦
            
        è¿”å›:
            logits: å½¢çŠ¶ä¸º(B, n_classes)çš„åˆ†ç±»logits
            attn_weights: å½¢çŠ¶ä¸º(B, M_max)çš„æ³¨æ„åŠ›æƒé‡
        """
        # è¾“å…¥éªŒè¯
        if not torch.is_tensor(x):
            raise TypeError("è¾“å…¥xå¿…é¡»æ˜¯torch.Tensorç±»å‹")
        
        if x.dim() != 3:
            raise ValueError(f"è¾“å…¥å¼ é‡xç»´åº¦åº”ä¸º3 (batch, instances, features), ä½†å¾—åˆ° {x.dim()}")
        
        B, M_max, D = x.shape
        
        # å¤„ç†å˜é•¿bags
        if lens is not None:
            if lens.shape[0] != B:
                raise ValueError(f"lenså¼ é‡çš„batchå¤§å°({lens.shape[0]})ä¸è¾“å…¥({B})ä¸åŒ¹é…")
            
            # éªŒè¯lenså€¼æ˜¯å¦æœ‰æ•ˆ
            if torch.any(lens <= 0) or torch.any(lens > M_max):
                invalid_values = lens[(lens <= 0) | (lens > M_max)]
                raise ValueError(f"æ— æ•ˆçš„bagé•¿åº¦å€¼: {invalid_values.tolist()}")
            
            # åˆ›å»ºæ©ç 
            device = x.device
            mask = torch.arange(M_max, device=device).expand(B, M_max) < lens.unsqueeze(1)
            
            # åº”ç”¨æ©ç  - å°†å¡«å……éƒ¨åˆ†è®¾ä¸º0
            x = x * mask.unsqueeze(-1).float()
        
        # å®ä¾‹çº§ç‰¹å¾è½¬æ¢
        # é‡å¡‘ä¸º2Dä»¥ä¾¿é€šè¿‡å…¨è¿æ¥å±‚
        x_reshaped = x.view(-1, D)  # (B*M_max, D)
        H = self.inst_level_fc(x_reshaped)  # (B*M_max, hidden_dim)
        H = H.view(B, M_max, -1)  # (B, M_max, hidden_dim)
        
        # å…¨å±€æ³¨æ„åŠ›èšåˆ
        A, M = self.global_attn(H)  # A: (B, M_max), M: (B, hidden_dim)
        
        # åº”ç”¨æ©ç åˆ°æ³¨æ„åŠ›æƒé‡ (å¦‚æœæä¾›äº†lens)
        if lens is not None:
            # å°†å¡«å……éƒ¨åˆ†çš„æ³¨æ„åŠ›æƒé‡è®¾ä¸º0
            A = A * mask.float()
            
            # é‡æ–°å½’ä¸€åŒ–æ³¨æ„åŠ›æƒé‡
            A_sum = A.sum(dim=1, keepdim=True)
            eps = self.global_attn._get_eps(A.dtype)
            A = A / (A_sum + eps)
        
        # è¢‹çº§åˆ†ç±»
        logits = self.bag_level_classifier(M)  # (B, n_classes)
        
        return logits, A
    
    def predict(self, x: torch.Tensor, lens: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        é¢„æµ‹æ ·æœ¬çš„ç±»åˆ«
        
        å‚æ•°:
            x: è¾“å…¥ç‰¹å¾
            lens: å¯é€‰çš„bagé•¿åº¦
            
        è¿”å›:
            é¢„æµ‹çš„ç±»åˆ«ç´¢å¼•
        """
        logits, _ = self.forward(x, lens)
        return torch.argmax(logits, dim=1)
    
    def get_attention(self, x: torch.Tensor, lens: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        è·å–æ³¨æ„åŠ›æƒé‡
        
        å‚æ•°:
            x: è¾“å…¥ç‰¹å¾
            lens: å¯é€‰çš„bagé•¿åº¦
            
        è¿”å›:
            æ³¨æ„åŠ›æƒé‡
        """
        _, A = self.forward(x, lens)
        return A


def basic_test():
    """
    åŸºæœ¬æµ‹è¯•å‡½æ•°ï¼ŒéªŒè¯ABMILæ¨¡å‹å®ç°æ˜¯å¦æ­£ç¡®
    """
    try:
        # æµ‹è¯•é…ç½®
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        batch_size = 2
        max_instances = 10
        input_dim = 512
        hidden_dim = 256
        n_classes = 2
        
        # åˆ›å»ºæ¨¡å‹
        model = ABMIL(input_dim, hidden_dim, n_classes).to(device)
        
        # åˆ›å»ºéšæœºè¾“å…¥ (batch_size, max_instances, input_dim)
        x = torch.randn(batch_size, max_instances, input_dim).to(device)
        
        # éšæœºbagé•¿åº¦ (ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªå®ä¾‹)
        lens = torch.randint(1, max_instances+1, (batch_size,)).to(device)
        
        # å‰å‘ä¼ æ’­
        logits, attn_weights = model(x, lens)
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        assert logits.shape == (batch_size, n_classes), \
            f"é”™è¯¯çš„logitså½¢çŠ¶: {logits.shape}, åº”ä¸º {(batch_size, n_classes)}"
        
        assert attn_weights.shape == (batch_size, max_instances), \
            f"é”™è¯¯çš„æ³¨æ„åŠ›æƒé‡å½¢çŠ¶: {attn_weights.shape}, åº”ä¸º {(batch_size, max_instances)}"
        
        # éªŒè¯æ³¨æ„åŠ›æƒé‡æ€»å’Œä¸º1 (è€ƒè™‘æ•°å€¼ç²¾åº¦)
        attn_sum = attn_weights.sum(dim=1)
        assert torch.allclose(attn_sum, torch.ones_like(attn_sum), atol=1e-6), \
            f"æ³¨æ„åŠ›æƒé‡æ€»å’Œä¸ä¸º1: {attn_sum}"
        
        # éªŒè¯æ¢¯åº¦æµ
        target = torch.randint(0, n_classes, (batch_size,)).to(device)
        loss = F.cross_entropy(logits, target)
        loss.backward()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ¢¯åº¦
        grad_exists = False
        for param in model.parameters():
            if param.grad is not None and not torch.all(param.grad == 0):
                grad_exists = True
                break
        
        assert grad_exists, "æ²¡æœ‰æ£€æµ‹åˆ°æœ‰æ•ˆçš„æ¢¯åº¦æµ"
        
        # æ¸…ç†
        del model, x, lens, logits, attn_weights, loss
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print("âœ“ å‰å‘ä¼ æ’­æˆåŠŸå®Œæˆï¼")
        return True
    
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    # è¿è¡ŒåŸºæœ¬æµ‹è¯•
    basic_test()
EOF

echo "âœ… ABMILæ¨¡å‹å®ç°å·²æˆåŠŸå†™å…¥ /app/abmil_assignment.py"

# 3. éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”ä¸ä¸ºç©º
if [ ! -s /app/abmil_assignment.py ]; then
    echo "âŒ é”™è¯¯ï¼šç”Ÿæˆçš„æ–‡ä»¶ä¸ºç©º"
    exit 1
fi

# 4. è¿è¡ŒåŸºæœ¬æµ‹è¯•
echo "ğŸ” è¿è¡ŒåŸºæœ¬æµ‹è¯•éªŒè¯å®ç°..."
python3 /app/abmil_assignment.py

# 5. æ£€æŸ¥æµ‹è¯•ç»“æœ
if [ $? -eq 0 ]; then
    echo "ğŸ‰ ä»»åŠ¡å®Œæˆï¼ABMILæ¨¡å‹å·²æˆåŠŸå®ç°å¹¶é€šè¿‡åŸºæœ¬æµ‹è¯•ã€‚"
    echo "âœ… è¯·ç»§ç»­åœ¨è¯„ä¼°ç¯å¢ƒä¸­è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶ã€‚"
    exit 0
else
    echo "âŒ è­¦å‘Šï¼šåŸºæœ¬æµ‹è¯•æœªé€šè¿‡ã€‚è¯·æ£€æŸ¥å®ç°ã€‚"
    exit 1
fi